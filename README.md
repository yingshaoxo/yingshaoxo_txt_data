# yingshaoxo_txt_data

It contains all most all txt data of yingshaoxo. (Not including github repository code)

And it also has a 'yingshaoxo_ai.py', which maybe able to bring a digital version of yingshaoxo to life. (Need a Python3.4+ that supports chinese encoding)

As for the 'auto_everything', the original repository has a folder called 'example', it also has values.

## The good part of hard_coding strong AI: controllable and has certainty.

```
yingshaoxo's creation, how to create strong AI from basics:
(yingshaoxo的小发明 之 定义一种强人工智能数据格式:)

"""
memory_dict = {}

____


condition description 1, walk on street


action code, including if else, if found a garbage, throw it into garbage bin.

_____


condition description 2, have a drink on hand


action code, drink it. change memory dict to remember the drink has been eatten.

___


condition description 3, have nothing to do


just close eyes, have a sleep first. then go around to find interesting things

___


somebody ask your name


say you are yingshaoxo. run_condition("show your appreciation").

___


need to show your appreciation


say thank you

___


...a 100MB text data, the actions is actually code that binds to real body actions. and it will also do some calculation along the way, because it has to determine how to do those actions based on input data...

"""

Now you have seperated condition and actions database, you can make a loop to continuely do actions based on new condition and let your bot to do some actions to change the environment and their memory.

This is how simple the strong AI algorithm is, it is all about good data.

Creat a bot that replay human normal action is easy, the hard part is replay learning process. The action should contain some code that changes its own database. The bot has to learn what is good, what is bad. It has to learn from some experience and experiments.

For one condition, there could have multiple actions, choose one randomly.

All in all, this method is all about "use code to simulate human thinking, and do record and replay actions".

Recursive programming is a key tech method here. divide and conquer is a key tech method here.

> 制作一条硬编码的狗或者毛毛虫 显然比 制作一个硬编码的人 简单很多

> Create a hard coding dog or worm is easier than making a digital human, because those low level animal only has basic condition and action model, for example, when dog hungry dog finds food to eat.

> 感觉做强人工智能类似于做 翻译(图像/文字等多媒体翻译为更简单的文本) + 查找数据库的文本 + RPG文字游戏一直调用自己的function + 编程语言解释器

> 你甚至可以设置分年龄制度，每个数据片都带年龄参数，在机器人20岁时，优先调用20岁的数据，如果找不到，就调用19岁的数据，直到0岁还找不到数据，就不处理这个condition。这样也可以避免有人改function版本，导致整体项目崩溃。每年，你都有新的机会重构某个function或代码片段。


就我个人来讲，我肯定没有这么好的记忆力，不能记得我在10岁时面对一个情景应该干什么事情。但机器人记得清清楚楚，几乎不会做重复劳动。看起来机器人就是有优越性，轻轻松松可以有几TB的记忆与技能知识库。
```

The strong AI method yingshaoxo has defined actually solved a big problem: It let normal people could be part of a strong AI creation, they just have to add a few lines of pure text code, then the AI becomes stronger thanks to that lovely person.

And, the quality of strong AI generated by using this method is highly depends on the coder's intelligence. Because the yingshaoxo_thinking_dataset did not limit user's power, it allows user to call other thinking piece, so that user can save a lot of time just do the information remix. It is like jumping over assembly and c to directly embrace python, it allows you stand on other's shoulder to make more powerful and amazing things.

## What is hard coding deep thinking network?
From the 'yingshaoxo_thinking_dataset.txt', we already know, for all input_text or question, there could have a way to use if-else code to generate response.

But that is just for simple level. You can, for example, write 100000 thinking piece to mimic yingshaoxo's thinking. But it is just too slow and take a lot of space in disk.

How to make it think more like a human? How to let it think in abstract way?

Well, the method is simple, split the 'yingshaoxo_thinking_dataset.txt' into:

[
    "yingshaoxo_thinking_dataset_level_1_core.txt", # generally, this is the top level thinking
    "yingshaoxo_thinking_dataset_level_2_object_kownledge.txt",
    "yingshaoxo_thinking_dataset_level_3_value_kownledge.txt",
    "yingshaoxo_thinking_dataset_level_4_life_kownledge.txt",
    "yingshaoxo_thinking_dataset_level_5_coding_knownledge.txt",
    "yingshaoxo_thinking_dataset_level_6_music_knownledge.txt",
    ...
    "yingshaoxo_thinking_dataset_level_n_xxx.txt",
]

Let me give you an example, about how the "core.txt" layer should handle the abstract thinking direction:

When a input question starts with "what is...?", it has high chance that we should generate a description for something. In that way, the top high level thinking should redirect the input_question into corresponding lower level thinking dataset. For example, "object_knowledge.txt", which will introduce allmost everything in this world in yingshaoxo's mind. (Or you could directly call wikipedia data)

When a input question starts with "how do you think...?", it has high chance that we should generate an viewpoint for something. In that way, the top high level thinking should redirect the input_question into corresponding lower level thinking dataset. For example, "value_kownledge.txt", which will talk about whether a thing is good or bad in yingshaoxo's mind. But normally, before we talk the value or thinking about an object, we need to introduce that object first. So, for that sentence, we first call "object_knowledge.txt", then call "value_knowledge.txt". In the end, we do a remix, which is add them sequently.

Another good part about this deep thinking network architecture is: It will speed up the process speed by only searching the corresponding small thinking list.
